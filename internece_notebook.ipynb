{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from skimage import color\n",
    "\n",
    "from dataset.utils.inception_utils import inception_resnet_v2_predict\n",
    "from dataset.utils.resize import resize_pad_frame\n",
    "from dataset.utils.shared import frames_per_video, default_nn_input_width, default_nn_input_height, resnet_input_height, resnet_input_width, dir_test, dir_test_results\n",
    "from model import FusionLayer\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(rc={'figure.figsize':(20,12)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video(file):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    file - path to video file \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    frames - frames array of the video\n",
    "    '''\n",
    "    \n",
    "    video = cv2.VideoCapture(file)\n",
    "    frames = []\n",
    "    while video.isOpened():\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    frames = np.asarray(frames)\n",
    "    return frames\n",
    "\n",
    "\n",
    "def get_lab_layer(frames):\n",
    "    '''\n",
    "        Parameters\n",
    "        -----------\n",
    "        frames - color/gray video frames with 3 chanels\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (rgb2lab, gray2lab) - RGB frames converted to LAB, GRAY frames converted to LAB\n",
    "    '''\n",
    "    rgb2lab_frames = []\n",
    "    gray2lab_frames = []\n",
    "\n",
    "    for frame in frames:\n",
    "        resized_frame = resize_pad_frame(frame, (default_nn_input_height, default_nn_input_width), equal_padding=True)\n",
    "\n",
    "        rgb2lab_frame = color.rgb2lab(resized_frame)\n",
    "        rgb2lab_frames.append(rgb2lab_frame)\n",
    "        \n",
    "        rgb2gray_frame = color.rgb2gray(resized_frame)\n",
    "        \n",
    "#         Display Grayscale frame\n",
    "#         cv2.imshow('grey', rgb2gray_frame)\n",
    "#         cv2.waitKey(0)\n",
    "#         cv2.destroyAllWindows()\n",
    "        \n",
    "        gray2rgb_frame = color.gray2rgb(rgb2gray_frame)\n",
    "        lab_frame = color.rgb2lab(gray2rgb_frame)\n",
    "        gray2lab_frames.append(lab_frame)\n",
    "\n",
    "    return np.asarray(rgb2lab_frames), np.asarray(gray2lab_frames)\n",
    "\n",
    "\n",
    "def preprocess_frames(gray2lab_frames):\n",
    "    '''\n",
    "    Parameters\n",
    "    ---------\n",
    "    gray2lab_frames - LAB frames of Grayscale video\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    processed_l_layer - L Layer processed (L/50 - 1)\n",
    "    '''\n",
    "    processed = np.empty(gray2lab_frames.shape)\n",
    "    \n",
    "    processed[:, :, :, 0] = np.divide(gray2lab_frames[:, :, :, 0], 50) - 1  # data loss\n",
    "    processed[:, :, :, 1] = np.divide(gray2lab_frames[:, :, :, 1], 128)\n",
    "    processed[:, :, :, 2] = np.divide(gray2lab_frames[:, :, :, 2], 128)\n",
    "    \n",
    "    processed_l_layer = processed[:, :, :, np.newaxis, 0]\n",
    "    \n",
    "    return processed_l_layer\n",
    "\n",
    "\n",
    "def get_resnet_records(frames):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    frames - original frames without color conversion or resizing\n",
    "    \n",
    "    Details\n",
    "    -------\n",
    "    Implementation adopted from Deep Kolorization implementation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    predictions - restnet predictions \n",
    "    '''\n",
    "    resnet_input = []\n",
    "    for frame in frames:\n",
    "        resized_frame = resize_pad_frame(frame, (resnet_input_height, resnet_input_width))\n",
    "        gray_scale_frame = cv2.cvtColor(resized_frame, cv2.COLOR_RGB2GRAY)\n",
    "        gray_scale_frame_colored = cv2.cvtColor(gray_scale_frame, cv2.COLOR_GRAY2RGB)\n",
    "        resnet_input.append(gray_scale_frame_colored)\n",
    "    resnet_input = np.asarray(resnet_input)\n",
    "\n",
    "    predictions = inception_resnet_v2_predict(resnet_input)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def getInputRange(frames_count, time_steps, current_frame):\n",
    "    '''\n",
    "    Deciding the moving window\n",
    "    '''\n",
    "    # this function should change according to our selection of\n",
    "    frame_selection = []\n",
    "    last_selection = current_frame\n",
    "    for i in range(current_frame, current_frame - time_steps, -1):\n",
    "        if (i < 0):\n",
    "            frame_selection.append(last_selection)\n",
    "        else:\n",
    "            frame_selection.append(i)\n",
    "            last_selection = i\n",
    "    frame_selection = frame_selection[::-1]\n",
    "    return frame_selection\n",
    "\n",
    "\n",
    "def get_nn_input(l_layer, resnet_out):\n",
    "    '''\n",
    "    Define the flowchroma input\n",
    "    '''\n",
    "    frames_count = l_layer.shape[0]\n",
    "    time_steps = frames_per_video\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(frames_count):\n",
    "        frame_index_selection = getInputRange(frames_count, time_steps, i)\n",
    "        frame_selection = []\n",
    "        resnet_selection = []\n",
    "        for j in frame_index_selection:\n",
    "            frame_selection.append(l_layer[j])\n",
    "            resnet_selection.append(resnet_out[j])\n",
    "        X.append(frame_selection)\n",
    "        Y.append(resnet_selection)\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(Y)\n",
    "    return [X, Y]\n",
    "\n",
    "\n",
    "def post_process_predictions(original_l_layers, predicted_AB_layers):\n",
    "    '''\n",
    "    Combine original L layer and predicted AB Layers\n",
    "    '''\n",
    "    time_steps = frames_per_video\n",
    "    total_frames = original_l_layers.shape[0]\n",
    "    predicted_frames = []\n",
    "    for i in range(total_frames):\n",
    "        l_layer = original_l_layers[i]\n",
    "        a_layer = np.multiply(predicted_AB_layers[i, time_steps - 1, :, :, 0], 128) # select the first frame outof three predictions\n",
    "        b_layer = np.multiply(predicted_AB_layers[i, time_steps - 1, :, :, 1], 128)\n",
    "        frame = np.empty((240, 320, 3))\n",
    "        frame[:, :, 0] = l_layer\n",
    "        frame[:, :, 1] = a_layer\n",
    "        frame[:, :, 2] = b_layer\n",
    "        #frame = color.lab2rgb(frame)\n",
    "        predicted_frames.append(frame)\n",
    "    return np.asarray(predicted_frames)\n",
    "\n",
    "\n",
    "def save_output_video(frames, output_file):\n",
    "    '''\n",
    "    Save the output video\n",
    "    '''\n",
    "    fps = 20\n",
    "    size = (320, 240)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, size)\n",
    "    count = 0\n",
    "    for frame in frames:\n",
    "        final_out = color.lab2rgb(frame)\n",
    "        #final_out = np.asarray(final_out).astype(int)\n",
    "        #plt.imshow(final_out)\n",
    "        final_out_write_video = final_out*255 # color.lab2rgb results values in [0,1]\n",
    "        final_out_write_video = final_out_write_video.astype(np.uint8)\n",
    "#         cv2.imshow('image', final_out)\n",
    "        count+=1\n",
    "#         cv2.waitKey(0)\n",
    "#         cv2.destroyAllWindows()\n",
    "        out.write(final_out_write_video)\n",
    "    out.release()\n",
    "\n",
    "    # write to output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running resnet model\n",
      "Combining L laber and resnet out\n"
     ]
    }
   ],
   "source": [
    "frames = get_video(\"cheetah_black.mp4\")\n",
    "(rgb2lab_frames, gray2lab_frames) = get_lab_layer(frames)\n",
    "processed_l_layer = preprocess_frames(gray2lab_frames)\n",
    "print('running resnet model')\n",
    "predictions = get_resnet_records(frames)\n",
    "print('Combining L laber and resnet out')\n",
    "X = get_nn_input(processed_l_layer, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 3, 240, 320,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv1 (TimeDistributed) (None, 3, 120, 160,  640         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv2 (TimeDistributed) (None, 3, 120, 160,  73856       encoder_conv1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv3 (TimeDistributed) (None, 3, 60, 80, 12 147584      encoder_conv2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv4 (TimeDistributed) (None, 3, 60, 80, 25 295168      encoder_conv3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv5 (TimeDistributed) (None, 3, 30, 40, 25 590080      encoder_conv4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv6 (TimeDistributed) (None, 3, 30, 40, 51 1180160     encoder_conv5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv7 (TimeDistributed) (None, 3, 30, 40, 51 2359808     encoder_conv6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv8 (TimeDistributed) (None, 3, 30, 40, 25 1179904     encoder_conv7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 3, 256)       0           encoder_conv8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "rnn_lstm1 (LSTM)                (None, 3, 256)       525312      time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "rnn_lstm2 (LSTM)                (None, 3, 256)       525312      rnn_lstm1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "inception_input (InputLayer)    (None, 3, 1000)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "rnn_dense1 (TimeDistributed)    (None, 3, 256)       65792       rnn_lstm2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "fusion_layer_4 (FusionLayer)    (None, 3, 30, 40, 15 0           encoder_conv8[0][0]              \n",
      "                                                                 inception_input[0][0]            \n",
      "                                                                 rnn_dense1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 3, 30, 40, 25 387328      fusion_layer_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_conv1 (TimeDistributed) (None, 3, 30, 40, 12 295040      time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_upsamp1 (TimeDistribute (None, 3, 60, 80, 12 0           decoder_conv1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_conv2 (TimeDistributed) (None, 3, 60, 80, 64 73792       decoder_upsamp1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_conv3 (TimeDistributed) (None, 3, 60, 80, 64 36928       decoder_conv2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_upsamp2 (TimeDistribute (None, 3, 120, 160,  0           decoder_conv3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_conv4 (TimeDistributed) (None, 3, 120, 160,  18464       decoder_upsamp2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_conv5 (TimeDistributed) (None, 3, 120, 160,  578         decoder_conv4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_upsamp3 (TimeDistribute (None, 3, 240, 320,  0           decoder_conv5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 7,755,746\n",
      "Trainable params: 7,755,746\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from model.flowchroma_network import FlowChroma\n",
    "from keras.layers import Input\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# run flowchroma model\n",
    "# ckpts = glob.glob(\"checkpoints/*.hdf5\")\n",
    "# latest_ckpt = max(ckpts, key=os.path.getctime)\n",
    "frames_per_video = 3\n",
    "default_nn_input_height = 240\n",
    "default_nn_input_width = 320\n",
    "\n",
    "time_steps, h, w = frames_per_video, default_nn_input_height, default_nn_input_width\n",
    "\n",
    "enc_input = Input(shape=(time_steps, h, w, 1), name='encoder_input')\n",
    "incep_out = Input(shape=(time_steps, 1000), name='inception_input')\n",
    "\n",
    "model = FlowChroma([enc_input, incep_out]).build()\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# generate_model_summaries(model)\n",
    "# print(\"loading from checkpoint:\", latest_ckpt)\n",
    "# model = load_model(latest_ckpt, custom_objects={'FusionLayer': FusionLayer})\n",
    "# predictions = []\n",
    "# for i in range(X[0].shape[0]):\n",
    "#     predictions.append(model.predict([X[0][i:i+1],X[1][i:i+1]])[0])\n",
    "# predictions = np.asarray(predictions)\n",
    "# print(\"Flowchroma model predictions calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(latest_ckpt, custom_objects={'FusionLayer': FusionLayer})\n",
    "predictions = []\n",
    "for i in range(X[0].shape[0]):\n",
    "    predictions.append(model.predict([X[0][i:i+1],X[1][i:i+1]])[0])\n",
    "predictions = np.asarray(predictions)\n",
    "print(\"Flowchroma model predictions calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_predictions = post_process_predictions(gray2lab_frames[:,:,:,0], predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_output_video(frame_predictions, dir_test_results+'/2.avi')\n",
    "# print(np.mean(frame_predictions[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rgb2lab_frames.shape)\n",
    "print(np.amax(rgb2lab_frames[0,:,:,0]))\n",
    "sns.heatmap(rgb2lab_frames[0,80:90,80:90,0],annot=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gray2lab_frames.shape)\n",
    "print(np.amax(gray2lab_frames[0,:,:,0]))\n",
    "sns.heatmap(gray2lab_frames[0,80:90,80:90,0],annot=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_l_layer.shape)\n",
    "print(np.amax(processed_l_layer[0,:,:,0]))\n",
    "sns.heatmap(processed_l_layer[0,80:90,80:90,0],annot=True )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L Layer Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax,ax2) = plt.subplots(ncols=2)\n",
    "fig.subplots_adjust(wspace=0.01)\n",
    "\n",
    "sns.heatmap(frame_predictions[0,80:90,80:90,0],annot=True, ax=ax )\n",
    "sns.heatmap(rgb2lab_frames[0,80:90,80:90,0],annot=True, ax=ax2 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Layer Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax,ax2) = plt.subplots(ncols=2)\n",
    "fig.subplots_adjust(wspace=0.01)\n",
    "sns.heatmap(frame_predictions[1,80:90,80:90,1],annot=True, ax=ax )\n",
    "sns.heatmap(rgb2lab_frames[1,80:90,80:90,1],annot=True, ax=ax2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B Layer comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax,ax2) = plt.subplots(ncols=2)\n",
    "fig.subplots_adjust(wspace=0.01)\n",
    "sns.heatmap(frame_predictions[0,80:90,80:90,2],annot=True, ax=ax )\n",
    "sns.heatmap(rgb2lab_frames[0,80:90,80:90,2],annot=True, ax=ax2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
